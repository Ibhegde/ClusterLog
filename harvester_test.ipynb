{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterlogs Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from clusterlogs import pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data from file and create pandas DataFrame with index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('samples/harvester_errors24.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/maria/cernbox/LogsClusterization/Harvester/data_sample.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/maria/cernbox/LogsClusterization/Harvester/data_sample-2020-02-27 15_01_06.750914.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/maria/cernbox/LogsClusterization/Harvester/data_sample_superror-2020-03-03 09_35_47.158324.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'message'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(df['message'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute clusterization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pipeline.Chain(df, target, mode='create', model_name='models/harvester_new.model', matching_accuracy=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pipeline.Chain(df, target, mode='process', model_name='models/harvester_30days.model', matching_accuracy=0.8, clustering_type='ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pipeline.Chain(df, target, mode='create', model_name='models/harvester_new.model', matching_accuracy=0.8, clustering_type='ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 equal groups\n",
      "Vectorization of tokens finished\n",
      "Vectorization of sentences is finished\n",
      "Vocabulary size = 190\n",
      "Number of dimensions is 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maria/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN finished with 25 clusters\n"
     ]
    }
   ],
   "source": [
    "cluster.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sequence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.groups.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Result: all clusters (big clusters and outliers) - sorted by cluster size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>sequence</th>\n",
       "      <th>tokenized_pattern</th>\n",
       "      <th>indices</th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>common_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Diag from worker : Condor HoldReason:        ;...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, Worke...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 1...</td>\n",
       "      <td>286</td>\n",
       "      <td>[(authorization failure worker canceled, 14.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diag from worker : Failed in data staging: Fai...</td>\n",
       "      <td>[Diag, from, worker, Failed, in, data, staging...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Failed, ▁,...</td>\n",
       "      <td>[158, 227, 245, 318, 332, 335, 148, 128, 130, ...</td>\n",
       "      <td>117</td>\n",
       "      <td>[(data staging failed, 8.524590163934427), (wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Diag from worker : LRMS error: (271) job killed:</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, job, killed]</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[34, 348, 356, 357, 358, 359, 361, 362, 363, 3...</td>\n",
       "      <td>104</td>\n",
       "      <td>[(diag, 1.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Diag from worker : Condor HoldReason:  error ;...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, error...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[372, 704, 759, 760, 761, 762, 763, 764, 765, ...</td>\n",
       "      <td>76</td>\n",
       "      <td>[(sigxcpu reason worker canceled, 14.5), (sigt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Diag from worker : LRMS error: (271) Job timeout</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, Job, timeout]</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[1, 83, 87, 95, 144, 175, 214, 225, 226, 228, ...</td>\n",
       "      <td>70</td>\n",
       "      <td>[(worker lrms error job timeout, 25.0), (diag,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Diag from worker : Condor HoldReason: None ; C...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, None,...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[8, 86, 111, 211, 464, 465, 466, 467, 468, 475...</td>\n",
       "      <td>58</td>\n",
       "      <td>[(worker condor holdreason, 9.0), (condor remo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Diag from worker : LRMS error: (-1) Job missin...</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, Job, missing...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[39, 40, 41, 43, 45, 46, 50, 52, 85, 114, 684,...</td>\n",
       "      <td>30</td>\n",
       "      <td>[(worker lrms error job missing, 25.0), (diag,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Diag from worker : Condor HoldReason:    ; Wor...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, Worke...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[13, 433, 435, 443, 738, 739, 740, 741, 781, 7...</td>\n",
       "      <td>14</td>\n",
       "      <td>[(worker condor holdreason job, 15.5), (found ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Diag from worker : Condor HoldReason: Network ...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, Netwo...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[135, 163, 165, 174, 197, 200, 201, 206, 215, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>[(authorization failure, 4.0), (diag, 1.0), (s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Diag from worker : Job submission to LRMS failed</td>\n",
       "      <td>[Diag, from, worker, Job, submission, to, LRMS...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Job, ▁, su...</td>\n",
       "      <td>[81, 131, 146, 374, 471, 613, 692, 757]</td>\n",
       "      <td>8</td>\n",
       "      <td>[(worker job submission, 9.0), (lrms failed, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Diag from worker : LRMS error: (381) Job was c...</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, Job, was, ca...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[38, 42, 44, 47, 48, 49, 558, 559]</td>\n",
       "      <td>8</td>\n",
       "      <td>[(worker lrms error job, 16.0), (diag, 1.0), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Diag from worker : Condor HoldReason: CREAM er...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, CREAM...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[90, 97, 98, 99, 100, 109, 110]</td>\n",
       "      <td>7</td>\n",
       "      <td>[(worker condor holdreason cream error, 25.0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Diag from worker : Error reading user generate...</td>\n",
       "      <td>[Diag, from, worker, Error, reading, user, gen...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Error, ▁, ...</td>\n",
       "      <td>[432, 436, 703, 798]</td>\n",
       "      <td>4</td>\n",
       "      <td>[(diag, 1.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Diag from worker : LRMS error: (-1) RemoveReas...</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, RemoveReason...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[800, 801, 132]</td>\n",
       "      <td>3</td>\n",
       "      <td>[(system_periodic_remove due, 4.0), (job runni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diag from worker : LRMS error: (1) Job failed</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, Job, failed]</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[84, 805, 807]</td>\n",
       "      <td>3</td>\n",
       "      <td>[(worker lrms error job failed, 25.0), (diag, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Diag from worker : Condor HoldReason: CREAM er...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, CREAM...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[353, 634]</td>\n",
       "      <td>2</td>\n",
       "      <td>[(sigxcpu reason, 4.0), (diag, 1.0), (terminat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Diag from worker : Condor HoldReason:  :      ...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, Worke...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[656, 813]</td>\n",
       "      <td>2</td>\n",
       "      <td>[(read proxy file worker canceled, 21.5), (wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diag from worker : Condor HoldReason:  ; Condo...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, Condo...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[277, 729]</td>\n",
       "      <td>2</td>\n",
       "      <td>[(condor removereason python initiated action,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Diag from worker : Condor HoldReason: HTCondor...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, HTCon...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[578, 682]</td>\n",
       "      <td>2</td>\n",
       "      <td>[(matching routes route job limit, 24.0), (rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Diag from worker : Condor HoldReason: CREAM er...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, CREAM...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[442]</td>\n",
       "      <td>1</td>\n",
       "      <td>[(error failed globus_l_gfs_file_open failed u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Diag from worker : LRMS error: (271) Node fail</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, Node, fail]</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[806]</td>\n",
       "      <td>1</td>\n",
       "      <td>[(worker lrms error node fail, 25.0), (diag, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diag from worker : Condor HoldReason: CREAM er...</td>\n",
       "      <td>[Diag, from, worker, Condor, HoldReason, CREAM...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...</td>\n",
       "      <td>[655]</td>\n",
       "      <td>1</td>\n",
       "      <td>[(socket timeout occurred worker canceled, 25....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Diag from worker : PODs=grid-job-86096-csnv7 ;...</td>\n",
       "      <td>[Diag, from, worker, PODs, container, termiate...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, PODs, =, g...</td>\n",
       "      <td>[660]</td>\n",
       "      <td>1</td>\n",
       "      <td>[(worker pods container termiated, 16.0), (rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Diag from worker : LRMS error: (-1) Job missin...</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, Job, missing...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[725]</td>\n",
       "      <td>1</td>\n",
       "      <td>[(worker lrms error job missing, 25.0), (data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Diag from worker : LRMS error: (-1) PeriodicRe...</td>\n",
       "      <td>[Diag, from, worker, LRMS, error, PeriodicRemo...</td>\n",
       "      <td>[Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...</td>\n",
       "      <td>[639]</td>\n",
       "      <td>1</td>\n",
       "      <td>[(worker lrms error periodicremove evaluated, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pattern  \\\n",
       "14  Diag from worker : Condor HoldReason:        ;...   \n",
       "0   Diag from worker : Failed in data staging: Fai...   \n",
       "10  Diag from worker : LRMS error: (271) job killed:    \n",
       "9   Diag from worker : Condor HoldReason:  error ;...   \n",
       "18   Diag from worker : LRMS error: (271) Job timeout   \n",
       "8   Diag from worker : Condor HoldReason: None ; C...   \n",
       "2   Diag from worker : LRMS error: (-1) Job missin...   \n",
       "20  Diag from worker : Condor HoldReason:    ; Wor...   \n",
       "11  Diag from worker : Condor HoldReason: Network ...   \n",
       "22   Diag from worker : Job submission to LRMS failed   \n",
       "17  Diag from worker : LRMS error: (381) Job was c...   \n",
       "16  Diag from worker : Condor HoldReason: CREAM er...   \n",
       "5   Diag from worker : Error reading user generate...   \n",
       "12  Diag from worker : LRMS error: (-1) RemoveReas...   \n",
       "4       Diag from worker : LRMS error: (1) Job failed   \n",
       "7   Diag from worker : Condor HoldReason: CREAM er...   \n",
       "6   Diag from worker : Condor HoldReason:  :      ...   \n",
       "3   Diag from worker : Condor HoldReason:  ; Condo...   \n",
       "24  Diag from worker : Condor HoldReason: HTCondor...   \n",
       "13  Diag from worker : Condor HoldReason: CREAM er...   \n",
       "15     Diag from worker : LRMS error: (271) Node fail   \n",
       "1   Diag from worker : Condor HoldReason: CREAM er...   \n",
       "19  Diag from worker : PODs=grid-job-86096-csnv7 ;...   \n",
       "21  Diag from worker : LRMS error: (-1) Job missin...   \n",
       "23  Diag from worker : LRMS error: (-1) PeriodicRe...   \n",
       "\n",
       "                                             sequence  \\\n",
       "14  [Diag, from, worker, Condor, HoldReason, Worke...   \n",
       "0   [Diag, from, worker, Failed, in, data, staging...   \n",
       "10     [Diag, from, worker, LRMS, error, job, killed]   \n",
       "9   [Diag, from, worker, Condor, HoldReason, error...   \n",
       "18    [Diag, from, worker, LRMS, error, Job, timeout]   \n",
       "8   [Diag, from, worker, Condor, HoldReason, None,...   \n",
       "2   [Diag, from, worker, LRMS, error, Job, missing...   \n",
       "20  [Diag, from, worker, Condor, HoldReason, Worke...   \n",
       "11  [Diag, from, worker, Condor, HoldReason, Netwo...   \n",
       "22  [Diag, from, worker, Job, submission, to, LRMS...   \n",
       "17  [Diag, from, worker, LRMS, error, Job, was, ca...   \n",
       "16  [Diag, from, worker, Condor, HoldReason, CREAM...   \n",
       "5   [Diag, from, worker, Error, reading, user, gen...   \n",
       "12  [Diag, from, worker, LRMS, error, RemoveReason...   \n",
       "4      [Diag, from, worker, LRMS, error, Job, failed]   \n",
       "7   [Diag, from, worker, Condor, HoldReason, CREAM...   \n",
       "6   [Diag, from, worker, Condor, HoldReason, Worke...   \n",
       "3   [Diag, from, worker, Condor, HoldReason, Condo...   \n",
       "24  [Diag, from, worker, Condor, HoldReason, HTCon...   \n",
       "13  [Diag, from, worker, Condor, HoldReason, CREAM...   \n",
       "15      [Diag, from, worker, LRMS, error, Node, fail]   \n",
       "1   [Diag, from, worker, Condor, HoldReason, CREAM...   \n",
       "19  [Diag, from, worker, PODs, container, termiate...   \n",
       "21  [Diag, from, worker, LRMS, error, Job, missing...   \n",
       "23  [Diag, from, worker, LRMS, error, PeriodicRemo...   \n",
       "\n",
       "                                    tokenized_pattern  \\\n",
       "14  [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "0   [Diag, ▁, from, ▁, worker, ▁, :, ▁, Failed, ▁,...   \n",
       "10  [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "9   [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "18  [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "8   [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "2   [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "20  [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "11  [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "22  [Diag, ▁, from, ▁, worker, ▁, :, ▁, Job, ▁, su...   \n",
       "17  [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "16  [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "5   [Diag, ▁, from, ▁, worker, ▁, :, ▁, Error, ▁, ...   \n",
       "12  [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "4   [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "7   [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "6   [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "3   [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "24  [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "13  [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "15  [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "1   [Diag, ▁, from, ▁, worker, ▁, :, ▁, Condor, ▁,...   \n",
       "19  [Diag, ▁, from, ▁, worker, ▁, :, ▁, PODs, =, g...   \n",
       "21  [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "23  [Diag, ▁, from, ▁, worker, ▁, :, ▁, LRMS, ▁, e...   \n",
       "\n",
       "                                              indices  cluster_size  \\\n",
       "14  [0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 1...           286   \n",
       "0   [158, 227, 245, 318, 332, 335, 148, 128, 130, ...           117   \n",
       "10  [34, 348, 356, 357, 358, 359, 361, 362, 363, 3...           104   \n",
       "9   [372, 704, 759, 760, 761, 762, 763, 764, 765, ...            76   \n",
       "18  [1, 83, 87, 95, 144, 175, 214, 225, 226, 228, ...            70   \n",
       "8   [8, 86, 111, 211, 464, 465, 466, 467, 468, 475...            58   \n",
       "2   [39, 40, 41, 43, 45, 46, 50, 52, 85, 114, 684,...            30   \n",
       "20  [13, 433, 435, 443, 738, 739, 740, 741, 781, 7...            14   \n",
       "11  [135, 163, 165, 174, 197, 200, 201, 206, 215, ...            12   \n",
       "22            [81, 131, 146, 374, 471, 613, 692, 757]             8   \n",
       "17                 [38, 42, 44, 47, 48, 49, 558, 559]             8   \n",
       "16                    [90, 97, 98, 99, 100, 109, 110]             7   \n",
       "5                                [432, 436, 703, 798]             4   \n",
       "12                                    [800, 801, 132]             3   \n",
       "4                                      [84, 805, 807]             3   \n",
       "7                                          [353, 634]             2   \n",
       "6                                          [656, 813]             2   \n",
       "3                                          [277, 729]             2   \n",
       "24                                         [578, 682]             2   \n",
       "13                                              [442]             1   \n",
       "15                                              [806]             1   \n",
       "1                                               [655]             1   \n",
       "19                                              [660]             1   \n",
       "21                                              [725]             1   \n",
       "23                                              [639]             1   \n",
       "\n",
       "                                       common_phrases  \n",
       "14  [(authorization failure worker canceled, 14.66...  \n",
       "0   [(data staging failed, 8.524590163934427), (wo...  \n",
       "10                                      [(diag, 1.0)]  \n",
       "9   [(sigxcpu reason worker canceled, 14.5), (sigt...  \n",
       "18  [(worker lrms error job timeout, 25.0), (diag,...  \n",
       "8   [(worker condor holdreason, 9.0), (condor remo...  \n",
       "2   [(worker lrms error job missing, 25.0), (diag,...  \n",
       "20  [(worker condor holdreason job, 15.5), (found ...  \n",
       "11  [(authorization failure, 4.0), (diag, 1.0), (s...  \n",
       "22  [(worker job submission, 9.0), (lrms failed, 4...  \n",
       "17  [(worker lrms error job, 16.0), (diag, 1.0), (...  \n",
       "16  [(worker condor holdreason cream error, 25.0),...  \n",
       "5                                       [(diag, 1.0)]  \n",
       "12  [(system_periodic_remove due, 4.0), (job runni...  \n",
       "4   [(worker lrms error job failed, 25.0), (diag, ...  \n",
       "7   [(sigxcpu reason, 4.0), (diag, 1.0), (terminat...  \n",
       "6   [(read proxy file worker canceled, 21.5), (wor...  \n",
       "3   [(condor removereason python initiated action,...  \n",
       "24  [(matching routes route job limit, 24.0), (rou...  \n",
       "13  [(error failed globus_l_gfs_file_open failed u...  \n",
       "15  [(worker lrms error node fail, 25.0), (diag, 1...  \n",
       "1   [(socket timeout occurred worker canceled, 25....  \n",
       "19  [(worker pods container termiated, 16.0), (rea...  \n",
       "21  [(worker lrms error job missing, 25.0), (data ...  \n",
       "23  [(worker lrms error periodicremove evaluated, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.in_cluster(cluster.result, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print only patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['  ', 'LRMS error: (271) job killed: ',\n",
       "       'LRMS error: (-1) Job missing from SLURM',\n",
       "       'LRMS error: (-1) Job missing from SLURM; Failed in data staging: Failed to prepare destination srm://srm.ndgf.org:8443/srm/managerv2?SFN=/atlas/disk/atlasdatadisk/rucio/mc16_13TeV//.:checksumtype=adler32:',\n",
       "       'LRMS error: (271) Job timeout',\n",
       "       \"Failed to create a JOB; HTTPSConnectionPool(host='', port=6443): Max retries exceeded with url: /apis/batch/v1/namespaces//jobs (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at >: Failed to establish a new connection: [Errno ] Connection ',))\",\n",
       "       'Killed by Harvester due to worker queuing too long.',\n",
       "       'LRMS error: (-1) RemoveReason: Job removed by SYSTEM_PERIODIC_REMOVE due to exceeding the requested memory.',\n",
       "       'Error reading user generated output file list',\n",
       "       'LRMS error: (271) Job was cancelled'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.result['pattern'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([('payload execution error returned', 16.0)]),\n",
       "       list([('job restarted undesirably', 9.0), ('condor removereason removed', 8.5), ('condor holdreason', 4.5), ('system_periodic_remove due', 4.0)]),\n",
       "       list([('submitted due', 4.0), ('incomplete data', 4.0), ('worker', 1.0)]),\n",
       "       list([('harvester due', 4.0), ('held', 1.0), ('long', 1.0), ('found', 1.0)]),\n",
       "       list([('harvester due', 4.0), ('held', 1.0), ('long', 1.0), ('found', 1.0)]),\n",
       "       list([('condor removereason python initiated action', 23.5), ('condor holdreason', 5.5), ('user atlpan', 4.0)]),\n",
       "       list([('condor holdreason cream error job', 25.0), ('sigxcpu reason worker canceled', 14.5), ('sigterm reason worker canceled', 14.5), ('sigxcpu worker canceled', 10.5), ('sigterm worker canceled', 10.5), ('harvester due', 4.0), ('terminated', 1.0), ('held', 1.0), ('long', 1.0), ('found', 1.0)]),\n",
       "       list([('moment faultcause submissions', 9.0), ('cream service', 4.0), ('accept jobs', 4.0)]),\n",
       "       list([('lrms error job killed wall', 25.0)]),\n",
       "       list([('moment faultcause threshold', 9.0), ('tomcat fd detected', 8.0), ('tomcat fd', 5.0), ('cream service', 4.0), ('accept jobs', 4.0), ('timestamp', 1.0)]),\n",
       "       list([('lrms error job killed vmem', 25.0)]),\n",
       "       list([('moment faultcause threshold', 9.0), ('ftp connection detected', 8.0), ('ftp connection', 5.0), ('cream service', 4.0), ('accept jobs', 4.0), ('tim', 1.0)]),\n",
       "       list([('condor holdreason job', 9.0), ('found worker canceled', 8.0), ('harvester due', 4.0), ('found', 2.0), ('held', 1.0), ('long', 1.0)]),\n",
       "       list([('lrms error job missing', 16.0), ('slurm', 1.0)]),\n",
       "       list([('sbatch worker canceled', 9.0), ('invalid missing', 4.0), ('limit error', 4.0), ('harvester due', 4.0), ('exceeds', 1.0), ('held', 1.0)]),\n",
       "       list([('condor removereason removed', 8.5), ('condor holdreason', 4.5), ('system_periodic_remove due', 4.0)]),\n",
       "       list([('true worker canceled', 9.0), ('condor holdreason', 4.0), ('harvester due', 4.0), ('held', 1.0), ('long', 1.0), ('found', 1.0)]),\n",
       "       list([('queue msg total number', 15.0), ('queue exceeds', 5.0), ('jobs', 1.0)]),\n",
       "       list([('queue msg total number', 15.0), ('queue limit user', 9.0), ('queue exceeds', 5.0), ('jobs', 1.0)]),\n",
       "       list([('queue limit user localhost qu', 23.666666666666664), ('queue msg total number', 15.666666666666666), ('queue exceeds', 5.666666666666666), ('jobs', 1.0)]),\n",
       "       list([('faultstring creamdelegationservice', 4.0), ('error', 1.0), ('due', 1.0)]),\n",
       "       list([('lrms error job missing', 16.0), ('data staging failed', 8.5), ('slurm failed', 4.5)]),\n",
       "       list([('lrms error job timeout', 16.0)]),\n",
       "       list([('harvester due', 4.0), ('held', 1.0), ('long', 1.0), ('found', 1.0)]),\n",
       "       list([('condor holdreason error receiving files', 23.5), ('target job schedd', 9.0), ('schedd transfer failed', 8.0), ('error reading', 5.5), ('send file', 4.0), ('failed', 2.0)]),\n",
       "       list([('harvester due', 4.0), ('worker queuing', 4.0), ('killed', 1.0), ('long', 1.0)]),\n",
       "       list([('condor holdreason network error talking', 25.0), ('authorization failure worker canceled', 16.0), ('harvester due', 4.0), ('schedd', 1.0), ('held', 1.0), ('long', 1.0), ('found', 1.0)]),\n",
       "       list([('condor holdreason unspecified gridmanager error', 25.0)]),\n",
       "       list([('error failed globus_l_gfs_file_open failed unable', 25.0), ('server responded', 4.0)]),\n",
       "       list([('condor holdreason error receiving files', 23.5), ('target job schedd', 9.0), ('schedd transfer failed', 8.0), ('error reading', 5.5), ('send file', 4.0), ('failed', 2.0)]),\n",
       "       list([('error failed globus_l_gfs_file_open failed unable', 25.0), ('open file sandboxes atlas', 16.0), ('server responded', 4.0)]),\n",
       "       list([('lrms error job missing', 16.0), ('data staging failed', 8.5), ('slurm failed', 4.5)]),\n",
       "       list([('connection errno connection refused', 16.0), ('newconnectionerror object', 4.0), ('failed', 1.0), ('create', 1.0), ('establish', 1.0)]),\n",
       "       list([('connection errno connection timed', 16.0), ('newconnectionerror object', 4.0), ('failed', 1.0), ('create', 1.0), ('establish', 1.0)]),\n",
       "       list([('matching routes route job limit', 24.0), ('route failure threshold', 10.0), ('harvester due', 4.0), ('held', 1.0), ('long', 1.0), ('found', 1.0)]),\n",
       "       list([('lrms error job', 9.0), ('cancelled', 1.0)]),\n",
       "       list([('lrms error removereason job removed', 25.0), ('system_periodic_remove due', 4.0), ('requested memory', 4.0), ('exceeding', 1.0)]),\n",
       "       list([]),\n",
       "       list([('read proxy file worker canceled', 23.5), ('condor holdreason failed', 9.0), ('proxy unable', 5.5), ('expiration time', 4.0), ('harvester due', 4.0), ('held', 1.0), ('long', 1.0), ('found', 1.0)]),\n",
       "       list([('error failed globus_l_gfs_file_open failed unable', 25.0), ('open file var cream_sandbox pilatlas', 25.0), ('server responded', 4.0)])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.result['common_phrases'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.in_cluster(cluster.result, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split clusters to big (cluster_size >= 1000) and small (cluster_size < 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big, small = cluster.split_clusters(cluster.result, 'cluster_size', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big['pattern'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all messages from cluster #40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.in_cluster(clusters, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the performance of all stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "text = '''Challenges in natural language processing frequently involve\n",
    "speech recognition, natural language understanding, natural language\n",
    "generation (frequently from formal, machine-readable logical forms),\n",
    "connecting language and machine perception, dialog systems, or some\n",
    "combination thereof.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tr = pytextrank.TextRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(text, min_count=1, threshold=1, scoring='npmi', delimiter=b' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "doc = nlp(text)\n",
    "for p in doc._.phrases:\n",
    "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
    "    print(p.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Automatic summarization is the process of reducing a text document with a \\\n",
    "computer program in order to create a summary that retains the most important points \\\n",
    "of the original document. As the problem of information overload has grown, and as \\\n",
    "the quantity of data has increased, so has interest in automatic summarization. \\\n",
    "Technologies that can make a coherent summary take into account variables such as \\\n",
    "length, writing style and syntax. An example of the use of summarization technology \\\n",
    "is search engines such as Google. Document summarization is another.\"\"\"\n",
    "text1 = \"\"\"\n",
    "Error on the surl while putdone. This SURL does not exist in the original request. \n",
    "\"\"\"\n",
    "\n",
    "from summa import summarizer\n",
    "print(summarizer.summarize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords.keywords(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cache_pos(strings, indexes):\n",
    "    factor = 1\n",
    "    pos = 0\n",
    "    for s, i in zip(strings, indexes):\n",
    "        pos += i * factor\n",
    "        factor *= len(s)\n",
    "    return pos\n",
    "\n",
    "def lcs_back(strings, indexes, cache):\n",
    "    if -1 in indexes:\n",
    "        return \"\"\n",
    "    match = all(strings[0][indexes[0]] == s[i]\n",
    "                for s, i in zip(strings, indexes))\n",
    "    if match:\n",
    "        new_indexes = [i - 1 for i in indexes]\n",
    "        result = lcs_back(strings, new_indexes, cache) + strings[0][indexes[0]]\n",
    "    else:\n",
    "        substrings = [\"\"] * len(strings)\n",
    "        for n in range(len(strings)):\n",
    "            if indexes[n] > 0:\n",
    "                new_indexes = indexes[:]\n",
    "                new_indexes[n] -= 1\n",
    "                cache_pos = calc_cache_pos(strings, new_indexes)\n",
    "                if cache[cache_pos] is None:\n",
    "                    substrings[n] = lcs_back(strings, new_indexes, cache)\n",
    "                else:\n",
    "                    substrings[n] = cache[cache_pos]\n",
    "        result = max(substrings, key=len)\n",
    "    cache[calc_cache_pos(strings, indexes)] = result\n",
    "    return result\n",
    "\n",
    "def lcs(strings):\n",
    "    \"\"\"\n",
    "    >>> lcs(['666222054263314443712', '5432127413542377777', '6664664565464057425'])\n",
    "    '54442'\n",
    "    >>> lcs(['abacbdab', 'bdcaba', 'cbacaa'])\n",
    "    'baa'\n",
    "    \"\"\"\n",
    "    if len(strings) == 0:\n",
    "        return \"\"\n",
    "    elif len(strings) == 1:\n",
    "        return strings[0]\n",
    "    else:\n",
    "        cache_size = 1\n",
    "        for s in strings:\n",
    "            cache_size *= len(s)\n",
    "        cache = [None] * cache_size\n",
    "        indexes = [len(s) - 1 for s in strings]\n",
    "        return lcs_back(strings, indexes, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs(['abacbdab', 'bdcaba', 'cbacaa', 'abacbdabaa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}