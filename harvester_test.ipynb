{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterlogs Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from clusterlogs import pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data from file and create pandas DataFrame with index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('samples/harvester_errors24.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/maria/cernbox/LogsClusterization/Harvester/data_sample.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/maria/cernbox/LogsClusterization/Harvester/data_sample-2020-02-27 15_01_06.750914.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/maria/cernbox/LogsClusterization/Harvester/data_sample_superror-2020-03-03 09_35_47.158324.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'message'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(df['message'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute clusterization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pipeline.Chain(df, target, mode='create', model_name='models/harvester_new.model', matching_accuracy=0.8, output_file='reports/harv_1day.html', clustering_type='ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pipeline.Chain(df, target, mode='process', model_name='models/harvester_30days.model', matching_accuracy=0.8, clustering_type='ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pipeline.Chain(df, target, mode='update', model_name='models/harvester_30days.model', matching_accuracy=0.8, clustering_type='ML',output_file='reports/harv_1day.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 equal groups\n",
      "Vectorization of tokens finished\n",
      "Vectorization of sentences is finished\n",
      "DBSCAN finished with 37 clusters\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009969]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009528]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.010136]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008883]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008684]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009771]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.010045]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009498]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009040]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009075]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008295]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008500]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008096]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007819]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008521]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007371]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008547]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009924]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007879]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007330]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008299]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007095]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007800]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007501]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007313]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007440]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007835]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009136]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008384]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.007267]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.012768]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.009501]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008827]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008807]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008870]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008630]\n",
      "Parsing data:\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.008864]\n"
     ]
    }
   ],
   "source": [
    "cluster.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sequence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.groups.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Result: all clusters (big clusters and outliers) - sorted by cluster size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pattern</th>\n",
       "      <th>indices</th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>common_phrases_RAKE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Payload execution error: returned non-zero]</td>\n",
       "      <td>[12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 2...</td>\n",
       "      <td>1010</td>\n",
       "      <td>[Payload execution error returned non zero]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[Condor HoldReason: None ; Condor RemoveReason...</td>\n",
       "      <td>[4, 5, 6, 44, 102, 103, 104, 106, 107, 108, 10...</td>\n",
       "      <td>792</td>\n",
       "      <td>[job restarted undesirably, system_periodic_re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[not submitted due to incomplete data of the w...</td>\n",
       "      <td>[31, 32, 33, 142, 151, 155, 156, 163, 164, 165...</td>\n",
       "      <td>362</td>\n",
       "      <td>[not submitted due, incomplete data, worker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[LRMS error: (271) job killed: &lt;*&gt;]</td>\n",
       "      <td>[168, 220, 221, 222, 282, 429, 487, 488, 489, ...</td>\n",
       "      <td>300</td>\n",
       "      <td>[lrms error job killed vmem, lrms error job ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[Condor HoldReason: Unspecified gridmanager er...</td>\n",
       "      <td>[0, 1, 2, 3, 91, 92, 93, 94, 95, 152, 153, 154...</td>\n",
       "      <td>235</td>\n",
       "      <td>[held too long, harvester due, not found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[Condor HoldReason: CREAM error: reason=(.*?) ...</td>\n",
       "      <td>[45, 46, 47, 48, 49, 50, 223, 224, 225, 226, 2...</td>\n",
       "      <td>234</td>\n",
       "      <td>[held too long, harvester due, not found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[Condor HoldReason: None ; Condor RemoveReason...</td>\n",
       "      <td>[40, 105, 138, 205, 206, 207, 232, 249, 326, 3...</td>\n",
       "      <td>214</td>\n",
       "      <td>[user atlpan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Condor HoldReason: CREAM error: Job has been ...</td>\n",
       "      <td>[263, 264, 265, 295, 466, 718, 885, 886, 887, ...</td>\n",
       "      <td>190</td>\n",
       "      <td>[held too long, harvester due, not found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[Condor HoldReason: CREAM error: CREAM_Job_Reg...</td>\n",
       "      <td>[25, 26, 27, 34, 35, 60, 61, 62, 63, 64, 65, 6...</td>\n",
       "      <td>183</td>\n",
       "      <td>[cream service cannot accept jobs, moment faul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Condor HoldReason: CREAM error: CREAM_Job_Reg...</td>\n",
       "      <td>[137, 143, 231, 281, 284, 353, 357, 362, 482, ...</td>\n",
       "      <td>148</td>\n",
       "      <td>[cream service cannot accept jobs, tomcat fd n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[Condor HoldReason: CREAM error: CREAM_Job_Reg...</td>\n",
       "      <td>[98, 99, 100, 101, 407, 408, 409, 528, 529, 53...</td>\n",
       "      <td>124</td>\n",
       "      <td>[cream service cannot accept jobs, ftp connect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[Condor HoldReason: Job not found ; Worker can...</td>\n",
       "      <td>[7, 36, 37, 38, 39, 144, 145, 146, 216, 283, 3...</td>\n",
       "      <td>117</td>\n",
       "      <td>[held too long, harvester due, not found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[LRMS error: (-1) Job missing from SLURM]</td>\n",
       "      <td>[238, 239, 240, 720, 721, 722, 723, 724, 725, ...</td>\n",
       "      <td>93</td>\n",
       "      <td>[lrms error job missing, slurm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[Condor HoldReason: submission command failed ...</td>\n",
       "      <td>[51, 52, 53, 54, 55, 139, 321, 416, 417, 418, ...</td>\n",
       "      <td>91</td>\n",
       "      <td>[exceeds some limit error, sbatch worker cance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[Condor HoldReason: CREAM error: BLAH error: s...</td>\n",
       "      <td>[8, 9, 10, 11, 126, 127, 141, 212, 340, 341, 4...</td>\n",
       "      <td>84</td>\n",
       "      <td>[queue msg total number, queue limit user, que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Condor HoldReason: None ; Condor RemoveReason...</td>\n",
       "      <td>[243, 244, 245, 246, 247, 443, 962, 963, 964, ...</td>\n",
       "      <td>49</td>\n",
       "      <td>[system_periodic_remove due]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Condor HoldReason: The system macro SYSTEM_PE...</td>\n",
       "      <td>[209, 327, 449, 655, 656, 685, 687, 821, 822, ...</td>\n",
       "      <td>45</td>\n",
       "      <td>[true worker canceled, held too long, condor h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[LRMS error: (-1) Job missing from SLURM; Fail...</td>\n",
       "      <td>[217, 218, 219, 308, 309, 310, 311, 452, 509, ...</td>\n",
       "      <td>41</td>\n",
       "      <td>[lrms error job missing, data staging failed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Condor HoldReason: CREAM error: BLAH error: s...</td>\n",
       "      <td>[803, 871, 872, 873, 874, 875, 876, 1019, 1020...</td>\n",
       "      <td>35</td>\n",
       "      <td>[queue msg total number, queue exceeds, jobs a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Condor HoldReason: CREAM_Delegate Error: Rece...</td>\n",
       "      <td>[41, 414, 579, 580, 768, 777, 778, 920, 1011, ...</td>\n",
       "      <td>35</td>\n",
       "      <td>[error, due]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[LRMS error: (271) Job timeout]</td>\n",
       "      <td>[439, 492, 519, 527, 1331, 1401, 1436, 1564, 1...</td>\n",
       "      <td>29</td>\n",
       "      <td>[lrms error job timeout]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Condor HoldReason: CREAM error: Stream closed...</td>\n",
       "      <td>[13, 714, 843, 1091, 1785, 1786, 1797, 1871, 1...</td>\n",
       "      <td>18</td>\n",
       "      <td>[held too long, harvester due, not found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Condor HoldReason: Error receiving files from...</td>\n",
       "      <td>[2509, 2510, 2511, 2513, 2517, 2518, 2519, 423...</td>\n",
       "      <td>18</td>\n",
       "      <td>[condor holdreason error receiving files, targ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[Killed by Harvester due to worker queuing too...</td>\n",
       "      <td>[896, 897, 898, 899, 900, 986, 1516, 1517, 181...</td>\n",
       "      <td>16</td>\n",
       "      <td>[worker queuing too long, harvester due, killed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Condor HoldReason: Unspecified gridmanager er...</td>\n",
       "      <td>[916, 1480, 1481, 1482, 1483, 2592, 2593, 3887...</td>\n",
       "      <td>13</td>\n",
       "      <td>[condor holdreason unspecified gridmanager error]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Condor HoldReason: Network error talking to s...</td>\n",
       "      <td>[562, 654, 1068, 1069, 1563, 2023, 2024, 2025,...</td>\n",
       "      <td>13</td>\n",
       "      <td>[condor holdreason network error talking, auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Condor HoldReason: Error receiving files from...</td>\n",
       "      <td>[1114, 1739, 2520, 2521, 3840, 4094, 4095, 423...</td>\n",
       "      <td>12</td>\n",
       "      <td>[condor holdreason error receiving files, targ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[Condor HoldReason: CREAM error: Transfer fail...</td>\n",
       "      <td>[403, 486, 517, 831, 1400, 1415, 2949, 3257, 3...</td>\n",
       "      <td>12</td>\n",
       "      <td>[server responded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[Condor HoldReason: CREAM error: Transfer fail...</td>\n",
       "      <td>[1099, 1705, 1980, 2438, 2439, 3172, 3555, 358...</td>\n",
       "      <td>10</td>\n",
       "      <td>[server responded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Failed to create a JOB; HTTPSConnectionPool(h...</td>\n",
       "      <td>[4150, 1703, 4151, 2348, 4450, 1704, 4366, 170...</td>\n",
       "      <td>9</td>\n",
       "      <td>[job port max retries exceeded, new connection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Failed to create a JOB; HTTPSConnectionPool(h...</td>\n",
       "      <td>[4049, 1680, 4051, 4331, 505, 4048, 4050, 3103...</td>\n",
       "      <td>9</td>\n",
       "      <td>[job port max retries exceeded, x f ca e faile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Condor HoldReason: HTCondor-CE held job due t...</td>\n",
       "      <td>[2709, 3859, 4273, 4274, 4275, 4378, 4429, 4430]</td>\n",
       "      <td>8</td>\n",
       "      <td>[held too long, harvester due, not found]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[Error reading user generated output file list]</td>\n",
       "      <td>[1827, 2115, 4198]</td>\n",
       "      <td>3</td>\n",
       "      <td>[Error reading user generated output file list]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[LRMS error: (271) Job was cancelled]</td>\n",
       "      <td>[866, 1757, 4347]</td>\n",
       "      <td>3</td>\n",
       "      <td>[lrms error job, cancelled]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[LRMS error: (-1) RemoveReason: Job removed by...</td>\n",
       "      <td>[1719, 2132, 4383]</td>\n",
       "      <td>3</td>\n",
       "      <td>[lrms error removereason job removed, system_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[Condor HoldReason: Failed to get expiration t...</td>\n",
       "      <td>[2795, 3491]</td>\n",
       "      <td>2</td>\n",
       "      <td>[read proxy file worker canceled, condor holdr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[Condor HoldReason: CREAM error: Transfer fail...</td>\n",
       "      <td>[2626]</td>\n",
       "      <td>1</td>\n",
       "      <td>[server responded]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pattern  \\\n",
       "9        [Payload execution error: returned non-zero]   \n",
       "20  [Condor HoldReason: None ; Condor RemoveReason...   \n",
       "27  [not submitted due to incomplete data of the w...   \n",
       "28                [LRMS error: (271) job killed: <*>]   \n",
       "24  [Condor HoldReason: Unspecified gridmanager er...   \n",
       "19  [Condor HoldReason: CREAM error: reason=(.*?) ...   \n",
       "34  [Condor HoldReason: None ; Condor RemoveReason...   \n",
       "5   [Condor HoldReason: CREAM error: Job has been ...   \n",
       "25  [Condor HoldReason: CREAM error: CREAM_Job_Reg...   \n",
       "8   [Condor HoldReason: CREAM error: CREAM_Job_Reg...   \n",
       "31  [Condor HoldReason: CREAM error: CREAM_Job_Reg...   \n",
       "35  [Condor HoldReason: Job not found ; Worker can...   \n",
       "32          [LRMS error: (-1) Job missing from SLURM]   \n",
       "36  [Condor HoldReason: submission command failed ...   \n",
       "30  [Condor HoldReason: CREAM error: BLAH error: s...   \n",
       "17  [Condor HoldReason: None ; Condor RemoveReason...   \n",
       "11  [Condor HoldReason: The system macro SYSTEM_PE...   \n",
       "16  [LRMS error: (-1) Job missing from SLURM; Fail...   \n",
       "2   [Condor HoldReason: CREAM error: BLAH error: s...   \n",
       "4   [Condor HoldReason: CREAM_Delegate Error: Rece...   \n",
       "21                    [LRMS error: (271) Job timeout]   \n",
       "13  [Condor HoldReason: CREAM error: Stream closed...   \n",
       "12  [Condor HoldReason: Error receiving files from...   \n",
       "29  [Killed by Harvester due to worker queuing too...   \n",
       "10  [Condor HoldReason: Unspecified gridmanager er...   \n",
       "3   [Condor HoldReason: Network error talking to s...   \n",
       "6   [Condor HoldReason: Error receiving files from...   \n",
       "26  [Condor HoldReason: CREAM error: Transfer fail...   \n",
       "22  [Condor HoldReason: CREAM error: Transfer fail...   \n",
       "0   [Failed to create a JOB; HTTPSConnectionPool(h...   \n",
       "1   [Failed to create a JOB; HTTPSConnectionPool(h...   \n",
       "14  [Condor HoldReason: HTCondor-CE held job due t...   \n",
       "23    [Error reading user generated output file list]   \n",
       "33              [LRMS error: (271) Job was cancelled]   \n",
       "7   [LRMS error: (-1) RemoveReason: Job removed by...   \n",
       "15  [Condor HoldReason: Failed to get expiration t...   \n",
       "18  [Condor HoldReason: CREAM error: Transfer fail...   \n",
       "\n",
       "                                              indices  cluster_size  \\\n",
       "9   [12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 2...          1010   \n",
       "20  [4, 5, 6, 44, 102, 103, 104, 106, 107, 108, 10...           792   \n",
       "27  [31, 32, 33, 142, 151, 155, 156, 163, 164, 165...           362   \n",
       "28  [168, 220, 221, 222, 282, 429, 487, 488, 489, ...           300   \n",
       "24  [0, 1, 2, 3, 91, 92, 93, 94, 95, 152, 153, 154...           235   \n",
       "19  [45, 46, 47, 48, 49, 50, 223, 224, 225, 226, 2...           234   \n",
       "34  [40, 105, 138, 205, 206, 207, 232, 249, 326, 3...           214   \n",
       "5   [263, 264, 265, 295, 466, 718, 885, 886, 887, ...           190   \n",
       "25  [25, 26, 27, 34, 35, 60, 61, 62, 63, 64, 65, 6...           183   \n",
       "8   [137, 143, 231, 281, 284, 353, 357, 362, 482, ...           148   \n",
       "31  [98, 99, 100, 101, 407, 408, 409, 528, 529, 53...           124   \n",
       "35  [7, 36, 37, 38, 39, 144, 145, 146, 216, 283, 3...           117   \n",
       "32  [238, 239, 240, 720, 721, 722, 723, 724, 725, ...            93   \n",
       "36  [51, 52, 53, 54, 55, 139, 321, 416, 417, 418, ...            91   \n",
       "30  [8, 9, 10, 11, 126, 127, 141, 212, 340, 341, 4...            84   \n",
       "17  [243, 244, 245, 246, 247, 443, 962, 963, 964, ...            49   \n",
       "11  [209, 327, 449, 655, 656, 685, 687, 821, 822, ...            45   \n",
       "16  [217, 218, 219, 308, 309, 310, 311, 452, 509, ...            41   \n",
       "2   [803, 871, 872, 873, 874, 875, 876, 1019, 1020...            35   \n",
       "4   [41, 414, 579, 580, 768, 777, 778, 920, 1011, ...            35   \n",
       "21  [439, 492, 519, 527, 1331, 1401, 1436, 1564, 1...            29   \n",
       "13  [13, 714, 843, 1091, 1785, 1786, 1797, 1871, 1...            18   \n",
       "12  [2509, 2510, 2511, 2513, 2517, 2518, 2519, 423...            18   \n",
       "29  [896, 897, 898, 899, 900, 986, 1516, 1517, 181...            16   \n",
       "10  [916, 1480, 1481, 1482, 1483, 2592, 2593, 3887...            13   \n",
       "3   [562, 654, 1068, 1069, 1563, 2023, 2024, 2025,...            13   \n",
       "6   [1114, 1739, 2520, 2521, 3840, 4094, 4095, 423...            12   \n",
       "26  [403, 486, 517, 831, 1400, 1415, 2949, 3257, 3...            12   \n",
       "22  [1099, 1705, 1980, 2438, 2439, 3172, 3555, 358...            10   \n",
       "0   [4150, 1703, 4151, 2348, 4450, 1704, 4366, 170...             9   \n",
       "1   [4049, 1680, 4051, 4331, 505, 4048, 4050, 3103...             9   \n",
       "14   [2709, 3859, 4273, 4274, 4275, 4378, 4429, 4430]             8   \n",
       "23                                 [1827, 2115, 4198]             3   \n",
       "33                                  [866, 1757, 4347]             3   \n",
       "7                                  [1719, 2132, 4383]             3   \n",
       "15                                       [2795, 3491]             2   \n",
       "18                                             [2626]             1   \n",
       "\n",
       "                                  common_phrases_RAKE  \n",
       "9         [Payload execution error returned non zero]  \n",
       "20  [job restarted undesirably, system_periodic_re...  \n",
       "27       [not submitted due, incomplete data, worker]  \n",
       "28  [lrms error job killed vmem, lrms error job ki...  \n",
       "24          [held too long, harvester due, not found]  \n",
       "19          [held too long, harvester due, not found]  \n",
       "34                                      [user atlpan]  \n",
       "5           [held too long, harvester due, not found]  \n",
       "25  [cream service cannot accept jobs, moment faul...  \n",
       "8   [cream service cannot accept jobs, tomcat fd n...  \n",
       "31  [cream service cannot accept jobs, ftp connect...  \n",
       "35          [held too long, harvester due, not found]  \n",
       "32                    [lrms error job missing, slurm]  \n",
       "36  [exceeds some limit error, sbatch worker cance...  \n",
       "30  [queue msg total number, queue limit user, que...  \n",
       "17                       [system_periodic_remove due]  \n",
       "11  [true worker canceled, held too long, condor h...  \n",
       "16  [lrms error job missing, data staging failed, ...  \n",
       "2   [queue msg total number, queue exceeds, jobs a...  \n",
       "4                                        [error, due]  \n",
       "21                           [lrms error job timeout]  \n",
       "13          [held too long, harvester due, not found]  \n",
       "12  [condor holdreason error receiving files, targ...  \n",
       "29   [worker queuing too long, harvester due, killed]  \n",
       "10  [condor holdreason unspecified gridmanager error]  \n",
       "3   [condor holdreason network error talking, auth...  \n",
       "6   [condor holdreason error receiving files, targ...  \n",
       "26                                 [server responded]  \n",
       "22                                 [server responded]  \n",
       "0   [job port max retries exceeded, new connection...  \n",
       "1   [job port max retries exceeded, x f ca e faile...  \n",
       "14          [held too long, harvester due, not found]  \n",
       "23    [Error reading user generated output file list]  \n",
       "33                        [lrms error job, cancelled]  \n",
       "7   [lrms error removereason job removed, system_p...  \n",
       "15  [read proxy file worker canceled, condor holdr...  \n",
       "18                                 [server responded]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.in_cluster(cluster.result, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print only patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster.result['pattern'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.result['common_phrases'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.in_cluster(cluster.result, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split clusters to big (cluster_size >= 1000) and small (cluster_size < 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big, small = cluster.split_clusters(cluster.result, 'cluster_size', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big['pattern'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all messages from cluster #40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.in_cluster(clusters, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the performance of all stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "text = '''Challenges in natural language processing frequently involve\n",
    "speech recognition, natural language understanding, natural language\n",
    "generation (frequently from formal, machine-readable logical forms),\n",
    "connecting language and machine perception, dialog systems, or some\n",
    "combination thereof.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tr = pytextrank.TextRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(text, min_count=1, threshold=1, scoring='npmi', delimiter=b' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "doc = nlp(text)\n",
    "for p in doc._.phrases:\n",
    "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
    "    print(p.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Automatic summarization is the process of reducing a text document with a \\\n",
    "computer program in order to create a summary that retains the most important points \\\n",
    "of the original document. As the problem of information overload has grown, and as \\\n",
    "the quantity of data has increased, so has interest in automatic summarization. \\\n",
    "Technologies that can make a coherent summary take into account variables such as \\\n",
    "length, writing style and syntax. An example of the use of summarization technology \\\n",
    "is search engines such as Google. Document summarization is another.\"\"\"\n",
    "text1 = \"\"\"\n",
    "Error on the surl while putdone. This SURL does not exist in the original request. \n",
    "\"\"\"\n",
    "\n",
    "from summa import summarizer\n",
    "print(summarizer.summarize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords.keywords(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cache_pos(strings, indexes):\n",
    "    factor = 1\n",
    "    pos = 0\n",
    "    for s, i in zip(strings, indexes):\n",
    "        pos += i * factor\n",
    "        factor *= len(s)\n",
    "    return pos\n",
    "\n",
    "def lcs_back(strings, indexes, cache):\n",
    "    if -1 in indexes:\n",
    "        return \"\"\n",
    "    match = all(strings[0][indexes[0]] == s[i]\n",
    "                for s, i in zip(strings, indexes))\n",
    "    if match:\n",
    "        new_indexes = [i - 1 for i in indexes]\n",
    "        result = lcs_back(strings, new_indexes, cache) + strings[0][indexes[0]]\n",
    "    else:\n",
    "        substrings = [\"\"] * len(strings)\n",
    "        for n in range(len(strings)):\n",
    "            if indexes[n] > 0:\n",
    "                new_indexes = indexes[:]\n",
    "                new_indexes[n] -= 1\n",
    "                cache_pos = calc_cache_pos(strings, new_indexes)\n",
    "                if cache[cache_pos] is None:\n",
    "                    substrings[n] = lcs_back(strings, new_indexes, cache)\n",
    "                else:\n",
    "                    substrings[n] = cache[cache_pos]\n",
    "        result = max(substrings, key=len)\n",
    "    cache[calc_cache_pos(strings, indexes)] = result\n",
    "    return result\n",
    "\n",
    "def lcs(strings):\n",
    "    \"\"\"\n",
    "    >>> lcs(['666222054263314443712', '5432127413542377777', '6664664565464057425'])\n",
    "    '54442'\n",
    "    >>> lcs(['abacbdab', 'bdcaba', 'cbacaa'])\n",
    "    'baa'\n",
    "    \"\"\"\n",
    "    if len(strings) == 0:\n",
    "        return \"\"\n",
    "    elif len(strings) == 1:\n",
    "        return strings[0]\n",
    "    else:\n",
    "        cache_size = 1\n",
    "        for s in strings:\n",
    "            cache_size *= len(s)\n",
    "        cache = [None] * cache_size\n",
    "        indexes = [len(s) - 1 for s in strings]\n",
    "        return lcs_back(strings, indexes, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs(['abacbdab', 'bdcaba', 'cbacaa', 'abacbdabaa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
