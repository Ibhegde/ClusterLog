{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterlogs Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from clusterlogs import multistage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download data from file and create pandas DataFrame with index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('samples/harvester_errors24.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CERN_central_B|111388583</td>\n",
       "      <td>Payload execution error: returned non-zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CERN_central_B|111388549</td>\n",
       "      <td>Payload execution error: returned non-zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>CERN_central_B|111388581</td>\n",
       "      <td>Payload execution error: returned non-zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CERN_central_B|111388573</td>\n",
       "      <td>Payload execution error: returned non-zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CERN_central_B|111389035</td>\n",
       "      <td>Payload execution error: returned non-zero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                                     message\n",
       "0  CERN_central_B|111388583  Payload execution error: returned non-zero\n",
       "1  CERN_central_B|111388549  Payload execution error: returned non-zero\n",
       "2  CERN_central_B|111388581  Payload execution error: returned non-zero\n",
       "3  CERN_central_B|111388573  Payload execution error: returned non-zero\n",
       "4  CERN_central_B|111389035  Payload execution error: returned non-zero"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize clusterization pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Required parameters are: \n",
    "- df - pandas DataFrame with error messages\n",
    "- target - name of a column with error messages\n",
    "\n",
    "Not required parameters are cluster_settings. If not defined - the default values are used.\n",
    "Note: It's better to use for big data samples with a vocabulary of more than 1000 tokens.\n",
    "\n",
    "CLUSTERING_DEFAULTS = {\"tokenizer\": \"nltk\",\n",
    "                       \"w2v_size\": \"auto\",\n",
    "                       \"w2v_window\": 5,\n",
    "                       \"min_samples\": 1}\n",
    "\n",
    "But you can define these parameters. For example,\n",
    "\n",
    "clustering_parameters = {'tokenizer':'nltk',\n",
    "                         'w2v_size': 200,\n",
    "                         'w2v_window': 5,\n",
    "                         'min_samples': 1}\n",
    "\n",
    "cluster = pipeline.ml_clustering(df, target, clustering_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'message'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = multistage.exec(df, target, 'harvester.model', threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result.big_clusters_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.small_clusters_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result._1.timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result._2.timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result._2.output.patterns.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.big_clusters.to_csv('harvester_big_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Payload execution error: returned non-zero', 'JOB not found',\n",
       "       'Condor HoldReason: CREAM error: BLAH error: submission command failed (exit code = 1) (stdout:) (stderr:qsub: number of jobs already in queue MSG=total number of jobs in queue exceeds the queue limit: user',\n",
       "       'submission failed: Exception OSError: [Errno 28] No space left on device',\n",
       "       'LRMS error: (271) job killed:',\n",
       "       \"Condor HoldReason: HTCondor-CE held job due to no matching routes, route job limit, or route failure threshold; see 'HTCondor-CE Troubleshooting Guide'; Worker canceled by harvester due to held too long or not found\",\n",
       "       'LRMS error: (-1) Job finished with unknown exit code',\n",
       "       'Condor HoldReason:; Worker canceled by harvester due to held too long or not found',\n",
       "       'Condor HoldReason: CREAM error: Transfer failed: globus_ftp_client: the server responded with an error 500 500-Command failed.: globus_l_gfs_file_open failed. 500-globus_xio: Unable to open file',\n",
       "       'not submitted due to incomplete data of the worker',\n",
       "       'Failed in data staging: Failed to prepare source Failed to prepare source: Resource',\n",
       "       \"Condor HoldReason: The system macro SYSTEM_PERIODIC_HOLD expression '(x509userproxysubject =?= undefined) || (x509UserProxyExpiration =?= undefined) || ((time()> x509UserProxyExpiration) && JobStatus =!= 3 && JobStatus =!= 4) || (RoutedBy =?= null && JobU\",\n",
       "       'LRMS error: (-1) Job missing from SLURM',\n",
       "       'Condor HoldReason: CREAM error: Job has been terminated (got; Worker canceled by harvester due to held too long or not found',\n",
       "       'Condor HoldReason: CREAM error: Cannot move ISB (retry_copy ${globus_transfer_cmd}',\n",
       "       'LRMS error: Job timeout',\n",
       "       \"Condor HoldReason: The system macro SYSTEM_PERIODIC_HOLD expression '( NumJobStarts >= 1 && JobStatus == 1 )' evaluated to TRUE; Worker canceled by harvester due to held too long or not found\",\n",
       "       'Condor HoldReason: ERROR: Failed to submit job; Worker canceled by harvester due to held too long or not found',\n",
       "       'Failed in data staging: Failed to prepare',\n",
       "       'Condor HoldReason: CREAM error: CREAM_Job_Register Error: MethodName=[jobRegister] ErrorCode=[0] Description=[The CREAM service cannot accept jobs at the moment] FaultCause=[Submissions are disabled!] Dec 2019; Worker canceled',\n",
       "       'Condor HoldReason: Error: Received NULL fault; the error is due to another cause: FaultString=[connection error] - FaultCode=[SOAP-ENV:Client] - FaultSubCode=[SOAP-ENV:Client] - FaultDetail=[Connection',\n",
       "       'Condor HoldReason: CREAM_Delegate Error: Connection to service [https://creamce3.goegrid.gwdg.de:8443/ce-cream/services/gridsite-delegation] failed: FaultString=[SSL error] - FaultCode=[SOAP-ENV:Client] - FaultSubCode=[SOAP-ENV:Client] - FaultDetail=[SSL a',\n",
       "       'Condor HoldReason: CREAM_Delegate Error: Received NULL fault; the error is due to another cause: FaultString=[The service cannot be found for the endpoint reference (EPR) https://clrccece01.in2p3.fr:8443/ce-cream/services/gridsite-delegation] - FaultCode=[',\n",
       "       'LRMS error: Job failed', 'LRMS error: Job failed with exit code',\n",
       "       'LRMS error: (-1) RemoveReason: Job removed by SYSTEM_PERIODIC_REMOVE due to exceeding the requested memory.',\n",
       "       'Condor HoldReason: Error receiving files from schedd osgserv06.slac.stanford.edu: DCSchedd::receiveJobSandbox:7003:File transfer failed for target job SCHEDD at 134.79.198.164 failed to send file(s) to error reading from',\n",
       "       'Condor HoldReason: globus_xio: Unable to connect to globus_xio: System error in connect: Connection timed out globus_xio: A system call failed: Connection timed out; Worker canceled by',\n",
       "       'Condor HoldReason: CREAM error: BLAH error: submission command failed (exit code = 1) (stdout:) (stderr:qsub: Bad UID for job execution MSG=could not authorize user atlasp from N/A (jobId =; Worker canceled by harvest',\n",
       "       \"Failed to create a JOB; HTTPSConnectionPool(host='127.0.0.1', port=6443): Max retries exceeded with url: /apis/batch/v1/namespaces/default/jobs (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at Failed to establish a new connection: [Errno 111] Connection refused',))\",\n",
       "       'LRMS error: (-1) ExitReason: died on signal 1 (Hangup)',\n",
       "       'Condor HoldReason: CREAM error: CREAM_Job_Register Error: MethodName=[jobRegister] ErrorCode=[0] Description=[The CREAM service cannot accept jobs at the moment] FaultCause=[Threshold for FTP Connection: 30 => Detected value for FTP Connection: Tim'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.big_clusters['pattern'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.outliers['cluster_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result.big_clusters['pattern'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.out.patterns['pattern'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.in_cluster(137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.in_cluster(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.in_cluster(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Execute clusterization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.w2v_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.output.stat_df.to_csv('harvester_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.output.in_cluster(5, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get clusters statistics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Clusters Statistics returns DataFrame with statistic for all clusters:\n",
    "- \"cluster_name\" - name of a cluster\n",
    "- \"cluster_size\" - number of log messages in cluster\n",
    "- \"pattern\" - all common substrings in messages in the cluster\n",
    "- \"vocab\" - all tokens in error messasges\n",
    "- \"vocab_length\" - the length of cluster's vocabulary\n",
    "- \"mean_length\" - average length of log messages in cluster\n",
    "- \"std_length\" - standard deviation of length of log messages in cluster\n",
    "- \"mean_similarity\" - average similarity of log messages in cluster\n",
    "(calculated as the levenshtein distances between the 1st and all other log messages)\n",
    "- \"std_similarity\" - standard deviation of similarity of log messages in cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cluster_output.Output(cluster.df, \n",
    "                               cluster.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'INDEX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.clustered_output(mode)\n",
    "stats = output.statistics(output_mode='frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats.sort_values(by=['mean_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = stats.sort_values(by='mean_similarity')[['pattern']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_1 = output.postprocessing(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_1.sort_values(by=['mean_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.in_cluster(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns1 = stats_1.sort_values(by='mean_similarity')[['pattern']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = output.stat_df.sort_values(by=['cluster_size'])[['cluster_size','cluster_name','pattern']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(df, new_clusters):\n",
    "    start = df.head(1)['pattern'].values[0]\n",
    "#     print(start)\n",
    "    ratio = [difflib.SequenceMatcher(None, start, x).ratio() for x in df['pattern']]\n",
    "    df['ratio'] = ratio\n",
    "    filtered = df[df['ratio'] > 0.6]['cluster_name'].values\n",
    "    new_clusters.append(filtered)\n",
    "#     print(filtered)\n",
    "    df.drop(df[df['cluster_name'].isin(filtered)].index, inplace=True)\n",
    "#     print(df)\n",
    "    while df.shape[0] > 0:\n",
    "        match(df, new_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters = []\n",
    "# start = sorted_df.head(1)['pattern'].values[0]\n",
    "# print(start)\n",
    "match(sorted_df, new_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in new_clusters:\n",
    "    x = cluster.df[cluster.df['cluster'].isin(i)].index\n",
    "    a.append({'cluster_name':i[-1],'idx':x})\n",
    "print(a)\n",
    "\n",
    "for i in a:\n",
    "    cluster.df.loc[i['idx'], 'cluster_level_2'] = i['cluster_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.df['cluster_level_2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted_df['pattern'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = sorted_list[0]\n",
    "print(start)\n",
    "ratio = []\n",
    "for x in sorted_list:\n",
    "    ratio.append(difflib.SequenceMatcher(None, start, x).ratio())\n",
    "print(ratio)\n",
    "ids = [i for i,x in enumerate(ratio) if x > 0.6]\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = output.postprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "rows = np.where(result > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings for all stages of clusterization pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process - timing of all process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.timings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all error messages in single cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.in_cluster(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.in_cluster(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output clusters - mode == 'ALL'  (for cluster '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.clustered_output(mode='ALL')['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output clusters - mode == 'INDEX' (for cluster '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.clustered_output(mode='INDEX')['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output clusters - mode == 'TARGET' (for cluster '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.clustered_output(mode='TARGET')['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output clusters - cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get epsilon value (which was used in DBSCAN algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
